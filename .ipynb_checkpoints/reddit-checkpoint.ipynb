{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aed0a740-04ea-4a67-ada9-e18bfb4a00b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "from data import *\n",
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import squarify\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import emoji    # removes emojis\n",
    "import re   # removes links\n",
    "import en_core_web_sm\n",
    "import string\n",
    "import yfinance as yf\n",
    "import pyfolio as pf\n",
    "from datetime import date\n",
    "from datetime import timedelta\n",
    "import json\n",
    "from sklearn.svm import SVR\n",
    "def data_extractor(reddit):\n",
    "    '''extracts all the data from reddit\n",
    "    Parameter: reddt: reddit obj\n",
    "    Return:    posts, c_analyzed, tickers, titles, a_comments, picks, subs, picks_ayz\n",
    "                \n",
    "                posts: int: # of posts analyzed\n",
    "                 c_analyzed: int: # of comments analyzed\n",
    "                 tickers: dict: all the tickers found\n",
    "                titles: list: list of the title of posts analyzed \n",
    "                 a_comments: dict: all the comments to analyze\n",
    "                 picks: int: top picks to analyze\n",
    "                 subs: int: # of subreddits analyzed\n",
    "                picks_ayz: int: top picks to analyze\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    '''############################################################################'''\n",
    "    # set the program parameters\n",
    "    subs = ['wallstreetbets','stocks' ]     # sub-reddit to search\n",
    "    post_flairs = {'Daily Discussion', 'Weekend Discussion', 'Discussion'}    # posts flairs to search || None flair is automatically considered\n",
    "    goodAuth = {'AutoModerator'}   # authors whom comments are allowed more than once\n",
    "    uniqueCmt = True                # allow one comment per author per symbol\n",
    "    ignoreAuthP = {'example'}       # authors to ignore for posts \n",
    "    ignoreAuthC = {'example'}       # authors to ignore for comment \n",
    "    upvoteRatio = 0.70         # upvote ratio for post to be considered, 0.70 = 70%\n",
    "    ups = 20       # define # of upvotes, post is considered if upvotes exceed this #\n",
    "    limit = 1     # define the limit, comments 'replace more' limit\n",
    "    upvotes = 2     # define # of upvotes, comment is considered if upvotes exceed this #\n",
    "    picks = 5     # define # of picks here, prints as \"Top ## picks are:\"\n",
    "    picks_ayz = 5   # define # of picks for sentiment analysis\n",
    "    '''############################################################################'''     \n",
    "    \n",
    "    posts, count, c_analyzed, tickers, titles, a_comments = 0, 0, 0, {}, [], {}\n",
    "    cmt_auth = {}\n",
    "    \n",
    "    for sub in subs:\n",
    "        subreddit = reddit.subreddit(sub)\n",
    "        hot_python = subreddit.hot()    # sorting posts by hot\n",
    "        # Extracting comments, symbols from subreddit\n",
    "        for submission in hot_python:\n",
    "            flair = submission.link_flair_text \n",
    "            author = submission.author.name         \n",
    "            \n",
    "            # checking: post upvote ratio # of upvotes, post flair, and author \n",
    "            if submission.upvote_ratio >= upvoteRatio and submission.ups > ups and (flair in post_flairs or flair is None) and author not in ignoreAuthP:   \n",
    "                submission.comment_sort = 'new'     \n",
    "                comments = submission.comments\n",
    "                titles.append(submission.title)\n",
    "                posts += 1\n",
    "                try: \n",
    "                    submission.comments.replace_more(limit=limit)   \n",
    "                    for comment in comments:\n",
    "                        # try except for deleted account?\n",
    "                        try: auth = comment.author.name\n",
    "                        except: pass\n",
    "                        c_analyzed += 1\n",
    "                        \n",
    "                        # checking: comment upvotes and author\n",
    "                        if comment.score > upvotes and auth not in ignoreAuthC:      \n",
    "                            split = comment.body.split(\" \")\n",
    "                            for word in split:\n",
    "                                word = word.replace(\"$\", \"\")        \n",
    "                                # upper = ticker, length of ticker <= 5, excluded words,                     \n",
    "                                if word.isupper() and len(word) <= 5 and word not in blacklist and word in us:\n",
    "                                    \n",
    "                                    # unique comments, try/except for key errors\n",
    "                                    if uniqueCmt and auth not in goodAuth:\n",
    "                                        try: \n",
    "                                            if auth in cmt_auth[word]: break\n",
    "                                        except: pass\n",
    "                                        \n",
    "                                    # counting tickers\n",
    "                                    if word in tickers:\n",
    "                                        tickers[word] += 1\n",
    "                                        a_comments[word].append(comment.body)\n",
    "                                        cmt_auth[word].append(auth)\n",
    "                                        count += 1\n",
    "                                    else:                               \n",
    "                                        tickers[word] = 1\n",
    "                                        cmt_auth[word] = [auth]\n",
    "                                        a_comments[word] = [comment.body]\n",
    "                                        count += 1   \n",
    "                except Exception as e: print(e)\n",
    "                \n",
    "                           \n",
    "    return posts, c_analyzed, tickers, titles, a_comments, picks, subs, picks_ayz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cfec054e-c753-4331-97f0-53a131d31ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_helper(tickers, picks, c_analyzed, posts, subs, titles, time, start_time):\n",
    "    '''prints out top tickers, and most mentioned tickers\n",
    "    \n",
    "    Parameter:   tickers: dict: all the tickers found\n",
    "                 picks: int: top picks to analyze\n",
    "                 c_analyzed: int: # of comments analyzed\n",
    "                 posts: int: # of posts analyzed\n",
    "                 subs: int: # of subreddits analyzed\n",
    "                titles: list: list of the title of posts analyzed \n",
    "                 time: time obj: top picks to analyze\n",
    "                start_time: time obj: prog start time\n",
    "\n",
    "    Return: symbols: dict: dict of sorted tickers based on mentions\n",
    "            times: list: include # of time top tickers is mentioned\n",
    "            top: list: list of top tickers\n",
    "    '''    \n",
    "\n",
    "    # sorts the dictionary\n",
    "    symbols = dict(sorted(tickers.items(), key=lambda item: item[1], reverse = True))\n",
    "    top_picks = list(symbols.keys())[0:picks]\n",
    "    time = (time.time() - start_time)\n",
    "    \n",
    "    # print top picks\n",
    "    print(\"It took {t:.2f} seconds to analyze {c} comments in {p} posts in {s} subreddits.\\n\".format(t=time, c=c_analyzed, p=posts, s=len(subs)))\n",
    "    print(\"Posts analyzed saved in titles\")\n",
    "    #for i in titles: print(i)  # prints the title of the posts analyzed\n",
    "    \n",
    "    \n",
    "    print(f\"\\n{picks} most mentioned tickers: \")\n",
    "    times = []\n",
    "    top = []\n",
    "    for i in top_picks:\n",
    "        print(f\"{i}: {symbols[i]}\")\n",
    "        times.append(symbols[i])\n",
    "        top.append(f\"{i}: {symbols[i]}\")\n",
    "   \n",
    "    return symbols, times, top\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bd11ad7d-bec5-4f68-8068-e64bf30d7f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install pyfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5a088228-d68e-48b5-91bc-eeddbd77db2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "def sentiment_analysis(picks_ayz, a_comments, symbols):\n",
    "    '''analyzes sentiment anaylsis of top tickers\n",
    "    \n",
    "    Parameter:   picks_ayz: int: top picks to analyze\n",
    "                 a_comments: dict: all the comments to analyze\n",
    "                 symbols: dict: dict of sorted tickers based on mentions\n",
    "    Return:      scores: dictionary: dictionary of all the sentiment analysis\n",
    "\n",
    "    '''\n",
    "    scores = {}\n",
    "     \n",
    "    vader = SentimentIntensityAnalyzer()\n",
    "    vader.lexicon.update(new_words)     # adding custom words from data.py \n",
    "    picks_sentiment = list(symbols.keys())[0:picks_ayz]\n",
    "    \n",
    "    for symbol in picks_sentiment:\n",
    "        stock_comments = a_comments[symbol]\n",
    "        for cmnt in stock_comments:\n",
    "    \n",
    "            emojiless = emoji.get_emoji_regexp().sub(u'', cmnt) # remove emojis\n",
    "            \n",
    "            # remove punctuation\n",
    "            text_punc  = \"\".join([char for char in emojiless if char not in string.punctuation])\n",
    "            text_punc = re.sub('[0-9]+', '', text_punc)\n",
    "                \n",
    "            # tokenizeing and cleaning \n",
    "            tokenizer = RegexpTokenizer('\\w+|\\$[\\d\\.]+|http\\S+')\n",
    "            tokenized_string = tokenizer.tokenize(text_punc)\n",
    "            lower_tokenized = [word.lower() for word in tokenized_string] # convert to lower case\n",
    "            \n",
    "            # remove stop words\n",
    "            nlp = en_core_web_sm.load()\n",
    "            stopwords = nlp.Defaults.stop_words\n",
    "            sw_removed = [word for word in lower_tokenized if not word in stopwords]\n",
    "            \n",
    "            # normalize the words using lematization\n",
    "            lemmatizer = WordNetLemmatizer()\n",
    "            lemmatized_tokens = ([lemmatizer.lemmatize(w) for w in sw_removed])\n",
    "            \n",
    "            # calculating sentiment of every word in comments n combining them\n",
    "            score_cmnt = {'neg': 0.0, 'neu': 0.0, 'pos': 0.0, 'compound': 0.0}\n",
    "            \n",
    "            word_count = 0\n",
    "            for word in lemmatized_tokens:\n",
    "                if word.upper() not in us:\n",
    "                    score = vader.polarity_scores(word)\n",
    "                    word_count += 1\n",
    "                    for key, _ in score.items():\n",
    "                        score_cmnt[key] += score[key]    \n",
    "                else:\n",
    "                    score_cmnt['pos'] = 2.0               \n",
    "                    \n",
    "            # calculating avg.\n",
    "            try:        # handles: ZeroDivisionError: float division by zero\n",
    "                for key in score_cmnt:\n",
    "                    score_cmnt[key] = score_cmnt[key] / word_count\n",
    "            except: pass\n",
    "                \n",
    "            \n",
    "            # adding score the the specific symbol\n",
    "            if symbol in scores:\n",
    "                for key, _ in score_cmnt.items():\n",
    "                    scores[symbol][key] += score_cmnt[key]\n",
    "            else:\n",
    "                scores[symbol] = score_cmnt        \n",
    "    \n",
    "        # calculating avg.\n",
    "        for key in score_cmnt:\n",
    "            scores[symbol][key] = scores[symbol][key] / symbols[symbol]\n",
    "            scores[symbol][key]  = \"{pol:.3f}\".format(pol=scores[symbol][key])\n",
    "            \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "861d4d90-ec37-4309-b18d-0509142f65e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment(scores,picks_ayz):\n",
    "    print(f\"\\nSentiment analysis of top {picks_ayz} picks:\")\n",
    "    df = pd.DataFrame(scores)\n",
    "    df.index = ['Bearish', 'Neutral', 'Bullish', 'Total/Compound']\n",
    "    df = df.T\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1a6704c2-5025-4044-98f5-698bbf0e72ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualization(picks_ayz, scores, picks, times, top):\n",
    "    '''prints sentiment analysis\n",
    "       makes a most mentioned picks chart\n",
    "       makes a chart of sentiment analysis of top picks\n",
    "       \n",
    "    Parameter:   picks_ayz: int: top picks to analyze\n",
    "                 scores: dictionary: dictionary of all the sentiment analysis\n",
    "                 picks: int: most mentioned picks\n",
    "                times: list: include # of time top tickers is mentioned\n",
    "                top: list: list of top tickers\n",
    "    Return:       None\n",
    "    '''\n",
    "    \n",
    "    # printing sentiment analysis \n",
    "    print(f\"\\nSentiment analysis of top {picks_ayz} picks:\")\n",
    "    df = pd.DataFrame(scores)\n",
    "    df.index = ['Bearish', 'Neutral', 'Bullish', 'Total/Compound']\n",
    "    df = df.T\n",
    "    print(df)\n",
    "    \n",
    "    # Date Visualization\n",
    "    # most mentioned picks    \n",
    "    squarify.plot(sizes=times, label=top, alpha=.7 )\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"{picks} most mentioned picks\")\n",
    "    #plt.show()\n",
    "    \n",
    "    # Sentiment analysis\n",
    "    df = df.astype(float)\n",
    "    colors = ['red', 'springgreen', 'forestgreen', 'coral']\n",
    "    df.plot(kind = 'bar', color=colors, title=f\"Sentiment analysis of top {picks_ayz} picks:\")\n",
    "    \n",
    "    \n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9d6e5f64-5871-4bb5-a2d7-ee7c09eb23e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SMA(data,period=30,column='Close'):\n",
    "    return data[column].rolling(window=period).mean()\n",
    "def EMA(data,period=20,column='Close'):\n",
    "    return data[column].ewm(span=period,adjust=False).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "22dcb521-89ad-4ced-b11c-f9d625fcdb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def StockPrice(symbols):\n",
    "    '''prints data frame  \n",
    "    Parameter:   symbols\n",
    "    Return:       data frame for top 5 sentiment analys stock price \n",
    "    '''\n",
    "    today_date = date.today()\n",
    "    three_Month = str(today_date -  timedelta(days=90))\n",
    "    period_long =26\n",
    "    period_short = 12\n",
    "    period_signal = 9\n",
    "    month_period = 30\n",
    "    sma_period = 30\n",
    "    #five = symbols.item()\n",
    "    top_5 = list(symbols.keys())[0:5]\n",
    "    df_price = pd.DataFrame()\n",
    "    for i in top_5:\n",
    "        i_data = yf.download(tickers= i, start= three_Month, interval='1d')\n",
    "        i_out_data = i_data.drop(columns = ['High','Low','Adj Close','Volume'])\n",
    "        i_out_data.rename(columns={'Close':i},inplace=True)\n",
    "        delta = i_out_data[i].diff(1)\n",
    "        delta = delta[1:]\n",
    "        up = delta.copy()\n",
    "        down = delta.copy()\n",
    "        up[up<0] = 0\n",
    "        down[down>0]=0\n",
    "        i_out_data['UP'] = up\n",
    "        i_out_data['down']=down\n",
    "        AVG_Gain = SMA(i_out_data,month_period,column = 'UP')\n",
    "        AVG_Loss = abs(SMA(i_out_data,month_period,column = 'down'))\n",
    "        RS = AVG_Gain / AVG_Loss\n",
    "        RSI = 100.0 - (100.0/(1.0 + RS))\n",
    "        i_out_data['RSI']=  RSI\n",
    "        i_out_data = i_out_data.drop(columns = ['UP','down'])\n",
    "        ShortEMA = EMA(i_out_data,period_short,column=i)\n",
    "        LongEMA = EMA(i_out_data,period_long,column = i)\n",
    "        i_out_data['MACD'] = ShortEMA - LongEMA\n",
    "        i_out_data['Signal_Line'] = EMA(i_out_data, period_signal ,column ='MACD')\n",
    "        i_out_data['SMA_30'] = SMA(i_out_data,sma_period,column=i)\n",
    "        std = i_out_data['SMA_30'].rolling(window= 30).std()\n",
    "        i_out_data['Upper_Band'] =i_out_data['SMA_30']+std*2\n",
    "        i_out_data['lower_Band'] =i_out_data['SMA_30']-std*2\n",
    "        i_out_data['Return'] = i_out_data['Open'].shift(-1)/i_out_data[i]\n",
    "        #print(i_out_data)\n",
    "        df_price = pd.concat([i_out_data,df_price],axis='columns',join='outer',ignore_index = False)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #df[i] = list(i_out_data)\n",
    "        #df.append(i_out_data)\n",
    "        df_price.dropna(inplace = True)\n",
    "    \n",
    "    return print(df_price.head())\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a305f58-45d4-4591-8f09-851e172c170b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e43b20f7-4319-4f77-b4ab-6a394030600d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#symbols ={'MRNA': 20, 'AMD': 15, 'ELY': 13, 'GME': 13, 'PFE': 10, 'AMZN': 9, 'MCFE': 8, 'PLTR': 7, 'FUBO': 6, 'MSFT': 6, 'AAPL': 5, 'CRSR': 4, 'NIO': 3, 'TSLA': 3, 'BB': 3, 'DKNG': 3, 'V': 3, 'WISH': 3, 'PYPL': 3, 'BNTX': 3, 'C': 2, 'MU': 2, 'R': 2, 'T': 2, 'CLNE': 2, 'UPST': 2, 'QS': 2, 'MVIS': 2, 'PRPL': 2, 'FSR': 2, 'M': 2, 'F': 2, 'SE': 2, 'ETSY': 2, 'PINS': 2, 'NVDA': 2, 'JPM': 2, 'BABA': 2, 'ASML': 2, 'APPS': 2, 'SQ': 2, 'ALB': 1, 'WEN': 1, 'BRO': 1, 'MARA': 1, 'TA': 1, 'CLF': 1, 'G': 1, 'ROKU': 1, 'SAVE': 1, 'PUBM': 1, 'DM': 1, 'RKT': 1, 'AN': 1, 'WKHS': 1, 'TLRY': 1, 'MS': 1, 'ET': 1, 'CASH': 1, 'EBAY': 1, 'IMVT': 1, 'ZNGA': 1, 'UGI': 1, 'Z': 1, 'RDFN': 1, 'LSPD': 1, 'NXPI': 1, 'KO': 1, 'JNJ': 1, 'AXP': 1, 'QCOM': 1, 'DVA': 1, 'UNH': 1, 'B': 1, 'AMAT': 1, 'GILD': 1, 'HBIO': 1, 'HOLX': 1, 'LITE': 1, 'RL': 1, 'CRM': 1, 'TGT': 1, 'PTON': 1, 'PKI': 1, 'BX': 1, 'GOOGL': 1, 'NDAQ': 1, 'MSCI': 1, 'BIO': 1, 'INFO': 1, 'TMO': 1, 'PJT': 1, 'CHTR': 1, 'TSCO': 1, 'POOL': 1, 'GNRC': 1, 'INMD': 1, 'CTAS': 1, 'SHOP': 1, 'MCO': 1, 'NXST': 1, 'PGR': 1, 'IDXX': 1, 'ZTS': 1, 'GM': 1, 'PLUG': 1, 'ABNB': 1, 'ASO': 1, 'O': 1, 'NKLA': 1, 'RNA': 1, 'TTD': 1, 'FSLY': 1, 'AI': 1, 'PRU': 1}\n",
    "#top_5 = list(symbols.keys())[0:4]\n",
    "#print(top_5)\n",
    "#StockPrice(symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a2537814-d432-4dbb-967b-161694fd9d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#StockPrice(symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "db6919a5-ac32-44f0-b6b2-7d9f18df3138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 31.49 seconds to analyze 1710 comments in 27 posts in 2 subreddits.\n",
      "\n",
      "Posts analyzed saved in titles\n",
      "\n",
      "5 most mentioned tickers: \n",
      "PLTR: 30\n",
      "WISH: 22\n",
      "DIS: 14\n",
      "TSLA: 8\n",
      "ROOT: 7\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "             Open  ROOT        RSI      MACD  Signal_Line    SMA_30  \\\n",
      "Date                                                                  \n",
      "2021-08-06  7.230  7.17  20.528777 -0.611557    -0.616953  8.549000   \n",
      "2021-08-09  7.240  7.56  25.869898 -0.566896    -0.606942  8.442667   \n",
      "2021-08-10  7.623  6.87  20.028612 -0.580488    -0.601651  8.303000   \n",
      "2021-08-11  6.890  6.87  20.802377 -0.584521    -0.598225  8.172000   \n",
      "\n",
      "            Upper_Band  lower_Band    Return        Open  ...    Return  \\\n",
      "Date                                                      ...             \n",
      "2021-08-06    9.794687    7.303312  1.009763  711.900024  ...  1.007121   \n",
      "2021-08-09    9.786182    7.099152  1.008333  710.169983  ...  1.002865   \n",
      "2021-08-10    9.743975    6.862025  1.002911  713.989990  ...  1.006796   \n",
      "2021-08-11    9.707076    6.636924  0.989811  712.710022  ...  1.014242   \n",
      "\n",
      "                 Open   PLTR        RSI      MACD  Signal_Line     SMA_30  \\\n",
      "Date                                                                        \n",
      "2021-08-06  22.639999  21.82  33.190575 -0.380287    -0.454153  23.047000   \n",
      "2021-08-09  21.850000  22.93  37.054472 -0.293614    -0.422045  22.918667   \n",
      "2021-08-10  23.010000  22.92  34.383760 -0.223159    -0.382268  22.770000   \n",
      "2021-08-11  23.070000  22.35  34.822702 -0.210886    -0.347992  22.627333   \n",
      "\n",
      "            Upper_Band  lower_Band    Return  \n",
      "Date                                          \n",
      "2021-08-06   24.034060   22.059940  1.001375  \n",
      "2021-08-09   23.979240   21.858094  1.003489  \n",
      "2021-08-10   23.930379   21.609621  1.006544  \n",
      "2021-08-11   23.898503   21.356164  1.093960  \n",
      "\n",
      "[4 rows x 45 columns]\n",
      "\n",
      "Sentiment analysis of top 5 picks:\n",
      "     Bearish Neutral Bullish Total/Compound\n",
      "PLTR   0.067   0.657   0.895          0.077\n",
      "WISH   0.113   0.785   0.646         -0.020\n",
      "DIS    0.106   0.728   0.533          0.020\n",
      "TSLA   0.064   0.709   0.719          0.049\n",
      "ROOT   0.154   0.701   0.319         -0.038\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    '''main function\n",
    "    Parameter:   None\n",
    "    Return:       None\n",
    "    '''\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # reddit client\n",
    "    reddit = praw.Reddit(user_agent=\"Comment Extraction\",\n",
    "                         client_id=\"VpHhcn4Pt6u42DfPBnbOrQ\",\n",
    "                         client_secret=\"YOFjc2YV0z3vxx0dU_eAT55VjprgJQ\",\n",
    "                         username=\"ayushshah1204\",\n",
    "                         password=\"Ayush@1204\")\n",
    "\n",
    "    posts, c_analyzed, tickers, titles, a_comments, picks, subs, picks_ayz = data_extractor(reddit)\n",
    "    symbols, times, top = print_helper(tickers, picks, c_analyzed, posts, subs, titles, time, start_time)\n",
    "    scores = sentiment_analysis(picks_ayz, a_comments, symbols)\n",
    "    stock_price = StockPrice(symbols)\n",
    "    sentiment(scores,picks_ayz)\n",
    "    \n",
    "    #visualization(picks_ayz, scores, picks, times, top)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1b7a55-1c29-481a-908a-5c91aae5c364",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b280bf-cc24-4ed6-92f5-267319863c81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
